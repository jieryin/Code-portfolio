---
title: "homework5"
author: "Jier Yin"
date: "2023-10-08"
output: pdf_document
---
# Conceptual 5-2
## a
The answer is 1-1/n

Bootstrap sampling involves drawing samples from the original data with replacement. For each bootstrap observation, any of the original n observations has an equal probability of being chosen.

Therefore, P(first bootstrap observation is the jth observation) = 1/n

P(first bootstrap observation is not the jth observation)= 1 - P(first bootstrap observation is the jth observation) = 1 - 1/n

## b
The answer is 1-1/n

## c
According to question a, P(an observation is not the jth observation) = 1-1/n. Since bootstrap sampling is done with replacement, the draws are independent of one another. 

So P(jth observation is not in the bootstrap sample) = (1-1/n)^n

## d
P = 1 - (1-1/5)^5 = 0.672

## e
P = 1 - (1-1/100)^100 = 0.634

## f
P = 1 - (1-1/10000)^10000 = 0.632

## g
```{r, 2g}
x <- 1:100000
plot(x, 1 - (1 - 1/x)^x)
```
When x is small, the curve is volatile and the value decreases really fast. As x increases, the curve quickly reaches an asymptote at about 0.632

## h
```{r, 2h}
store <- rep(NA, 10000)
for (i in 1:10000) {
    store[i] <- sum(sample(1:100, rep = TRUE) == 4) > 0
}
mean(store)
```
$$
\lim_{{n \to \infty}} (1 + \frac{x}{n})^n = e^x
$$
When x = -1, theoretically, a bootstrap sample of size n contains the jth observation converges to 1-1/e=0.632 as n increases to infinite. The result from the given loop is closely align with our theoretical expectation of 0.632.

# Conceptual 5-3
## a
k-fold cross-validation operates by partitioning the n observations into k distinct, non-overlapping subsets, each of roughly equal size n/k. For each subset, it is treated as a validation set while the remaining data, which amounts to n-n/k observations, serves as the training set. The model is trained and validated k times, rotating the validation set each time. Finally, to evaluate the model's performance, the k Mean Squared Error (MSE) outcomes from these iterations are averaged, providing an aggregate test error estimate.

## b
-The validation set approach?

The validation set approach can exhibit substantial variability in the test error estimate. This is contingent upon which observations are designated for the training versus the validation set. Conversely, k-fold cross-validation mitigates this variability by averaging results over multiple partitions.

In the validation set approach, only a fraction of the observations—those in the training set—are used to train the model. As statistical methods often exhibit poorer performance when trained on limited data, the validation set error rate can sometimes provide an overestimation of the test error rate, especially when contrasted with a model trained on the entire dataset. k-fold cross-validation, on the other hand, uses every observation for both training and validation across its iterations.

-LOOCV?

LOOCV necessitates the model to be trained n times, once for each data point. This can be computationally intense, especially for large datasets. In comparison, k-fold cross-validation, with smaller values of k, is more computationally efficient, requiring only k model trainings.

LOOCV can yield nearly unbiased test error estimates since every training set comprises n-1 observations. However, it tends to exhibit higher variance relative to k-fold cross-validation. The reason being, the outputs of the n models in LOOCV, trained on almost identical data sets, are strongly correlated. Averaging highly correlated outputs results in a higher variance. In the realm of k-fold cross-validation, selecting values like k=5 or k=10 often produces test error estimates that strike a balance, avoiding both excessive bias and high variance.

# Conceptual 5-4
To estimate the standard deviation of our prediction, the bootstrap technique can be employed. Instead of acquiring new independent datasets from the population to fit our model, we rely on repeated random sampling from our initial dataset. This involves drawing samples with replacement B times. With each sample, we compute the respective estimates. The standard deviation of these B estimates then provides an insight into the variability or standard deviation of our prediction.

We may use k-fold Cross-validation Method. The dataset is partitioned into k distinct subsets. For every iteration, the model gets trained on k-1 subsets and validated on the leftover subset. This process is carried out k times, each with a unique subset serving as the validation set. This results in k distinct predictions for the predictor value X. The standard deviation of these accumulated predictions acts as an indicative estimate of the prediction's variability.

# Applied 5-5
## a
```{r, 5a}
library(ISLR) 
set.seed(1)
data(Default)
logit_model <- glm(default ~ income + balance, data=Default, family="binomial")
summary(logit_model)
```
## b
```{r, 5b}
#i
train <- sample(dim(Default)[1], dim(Default)[1] / 2)

#ii
model <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
summary(model)

#iii
predicted_probs <- predict(model, newdata = Default[-train, ], type = "response")
predicted_default <- ifelse(predicted_probs > 0.5, "Yes", "No")

#iv
mean(predicted_default != Default[-train, ]$default)
```
## c
```{r, 5c}
compute_validation_error <- function() {
  train <- sample(dim(Default)[1], dim(Default)[1] / 2)
  model <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
  predicted_probs <- predict(model, newdata = Default[-train, ], type = "response")
  predicted_default <- ifelse(predicted_probs > 0.5, "Yes", "No")
  return(mean(predicted_default != Default[-train, ]$default))
}
set.seed(123)
compute_validation_error()
set.seed(124)
compute_validation_error()
set.seed(125)
compute_validation_error()
```
The validation estimate of the test error rate is variable, depending on precisely which observations are included in the training set and which observations are included in the validation set. The validation set error will vary with each different split due to the random nature of the sampling process. But the values we got are close.

## d
```{r, 5d}
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train)
pred.glm <- rep("No", length(predicted_probs))
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
```
It doesn’t seem that adding the “student” dummy variable leads to a reduction in the validation set estimate of the test error rate.

# Applied 5-7
## a
```{r, 7a}
model <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = "binomial")
summary(model)
```

## b
```{r, 7b}
model_without_first <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = "binomial")
summary(model_without_first)
```
## c
```{r, 7c}
predict(model_without_first, Weekly[1, ], type = "response") > 0.5
```
The prediction for the first observation is “Up”. This observation was not correctly classified as the true direction is “Down”.

## d
```{r, 7d}
error <- rep(0, dim(Weekly)[1])
for (i in 1:dim(Weekly)[1]) {
    model <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ],  family = "binomial")
    pred.up <- predict(model, Weekly[i, ], type = "response") > 0.5
    true.up <- Weekly[i, ]$Direction == "Up"
    if (pred.up != true.up)
        error[i] <- 1
}
error
```
## e
```{r, 7e}
mean(error)
```
The LOOCV estimate for the test error rate is 44.9954086%. This is a substantial error rate, especially for a binary classification problem where a random guessing would result in an expected error rate of 50%. Given that our model is only performing slightly better than random guessing, it suggests that the logistic regression model using "Lag1" and "Lag2" as predictors might not be capturing the underlying patterns of the data effectively.

# Applied 5-8
## a
```{r, 8a}
set.seed(1)
y <- rnorm(100)
x <- rnorm(100)
y <- x - 2 * x^2 + rnorm(100)
```
The n is 100 and the p is 1.
The model is 
$$
Y = X - 2X^2 + \epsilon
$$
## b
```{r, 8b}
plot(x, y)
```
The plot suggests that there is a curved relationship between x and y. When x is less than 0.25, y increases as x increases; when x is larger than 0.25, y decreases as x increases. 

## c
```{r, 8c}
#i
library(boot)
set.seed(1)
Data <- data.frame(x, y)
fit.glm.1 <- glm(y ~ x)
cv.glm(Data, fit.glm.1)$delta[1]

#ii
fit.glm.2 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.glm.2)$delta[1]

#iii
fit.glm.3 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit.glm.3)$delta[1]

#iv
fit.glm.4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit.glm.4)$delta[1]
```
## d
```{r, 8d}
#i
set.seed(10)
fit.glm.1 <- glm(y ~ x)
cv.glm(Data, fit.glm.1)$delta[1]

#ii
fit.glm.2 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.glm.2)$delta[1]

#iii
fit.glm.3 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit.glm.3)$delta[1]

#iv
fit.glm.4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit.glm.4)$delta[1]

```
The results are the same. This is because you always leave out one observation, fit the model, and predict, there's no randomness involved in LOOCV once you have the data. Therefore, the LOOCV errors for a given dataset will always be the same regardless of the random seed.

## e
The LOOCV estimate for the test MSE is minimum for “fit.glm.2”. Yes, this is what I expect because as we can see in (a) and (b) the relation between “x” and “y” is quadratic.

## f
```{r, 8f}
summary(fit.glm.4)
```
The summary shows that the linear and quadratic terms are statistically significants and that the cubic and 4th degree terms are not statistically significants. As the LOOCV error was lowest for the quadratic model, this is in agreement with the conclusion from the hypothesis tests that the significant terms are the intercept, linear, and quadratic ones.

# Applied 5-9
## a
```{r, 9a}
library(MASS)
attach(Boston)
mu.hat <- mean(medv)
mu.hat
```

## b
```{r, 9b}
se.hat <- sd(medv) / sqrt(length(Boston$medv))
se.hat
```
The standard error of mu-hat is 0.4088611. This means that, on average, we would expect the sample mean of "medv" to deviate from the true population mean by about 0.4088611 units. 

## c
```{r, 9c}
set.seed(1)
boot.fn <- function(data, index) {
    mu <- mean(data[index])
    return (mu)
}
boot(medv, boot.fn, 1000)
```
The standard error we got this time is 0.4107, which is very close to the standard error we got in (b).

## d
```{r, 9d}
t.test(medv)
CI.mu.hat <- c(22.53 - 2 * 0.4107, 22.53 + 2 * 0.4107)
CI.mu.hat
```
The bootstrap confidence interval is very close to the one provided by the t.test() function.

## e
```{r, 9e}
med.hat <- median(medv)
med.hat
```

## f
```{r, 9f}
boot.fn <- function(data, index) {
    mu <- median(data[index])
    return (mu)
}
boot(medv, boot.fn, 1000)
```
We get an estimated median value of 21.2 which is equal to the value obtained in (e). And a standard error of 0.3688 which is relatively small compared to median value. This suggests that the median value is estimated with a good degree of precision.

## g
```{r, 9g}
percent.hat <- quantile(medv, c(0.1))
percent.hat
```

## h
```{r, 9h}
boot.fn <- function(data, index) {
    mu <- quantile(data[index], c(0.1))
    return (mu)
}
boot(medv, boot.fn, 1000)
```
In this question, we get an estimated tenth percentile value of 12.75 which is  equal to the value we got in (g), with a standard error of 0.5113 which is relatively small compared to percentile value. This suggests that the tenth percentile value is estimated with a good degree of precision.




