# Applied 5-5
## a
```{r, 5a}
library(ISLR) 
set.seed(1)
data(Default)
logit_model <- glm(default ~ income + balance, data=Default, family="binomial")
summary(logit_model)
```
## b
```{r, 5b}
#i
train <- sample(dim(Default)[1], dim(Default)[1] / 2)

#ii
model <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
summary(model)

#iii
predicted_probs <- predict(model, newdata = Default[-train, ], type = "response")
predicted_default <- ifelse(predicted_probs > 0.5, "Yes", "No")

#iv
mean(predicted_default != Default[-train, ]$default)
```
## c
```{r, 5c}
compute_validation_error <- function() {
  train <- sample(dim(Default)[1], dim(Default)[1] / 2)
  model <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
  predicted_probs <- predict(model, newdata = Default[-train, ], type = "response")
  predicted_default <- ifelse(predicted_probs > 0.5, "Yes", "No")
  return(mean(predicted_default != Default[-train, ]$default))
}
set.seed(123)
compute_validation_error()
set.seed(124)
compute_validation_error()
set.seed(125)
compute_validation_error()
```
The validation estimate of the test error rate is variable, depending on precisely which observations are included in the training set and which observations are included in the validation set. The validation set error will vary with each different split due to the random nature of the sampling process. But the values we got are close.

## d
```{r, 5d}
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train)
pred.glm <- rep("No", length(predicted_probs))
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
```
It doesn’t seem that adding the “student” dummy variable leads to a reduction in the validation set estimate of the test error rate.

# Applied 5-7
## a
```{r, 7a}
model <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = "binomial")
summary(model)
```

## b
```{r, 7b}
model_without_first <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = "binomial")
summary(model_without_first)
```
## c
```{r, 7c}
predict(model_without_first, Weekly[1, ], type = "response") > 0.5
```
The prediction for the first observation is “Up”. This observation was not correctly classified as the true direction is “Down”.

## d
```{r, 7d}
error <- rep(0, dim(Weekly)[1])
for (i in 1:dim(Weekly)[1]) {
    model <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ],  family = "binomial")
    pred.up <- predict(model, Weekly[i, ], type = "response") > 0.5
    true.up <- Weekly[i, ]$Direction == "Up"
    if (pred.up != true.up)
        error[i] <- 1
}
error
```
## e
```{r, 7e}
mean(error)
```
The LOOCV estimate for the test error rate is 44.9954086%. This is a substantial error rate, especially for a binary classification problem where a random guessing would result in an expected error rate of 50%. Given that our model is only performing slightly better than random guessing, it suggests that the logistic regression model using "Lag1" and "Lag2" as predictors might not be capturing the underlying patterns of the data effectively.

# Applied 5-8
## a
```{r, 8a}
set.seed(1)
y <- rnorm(100)
x <- rnorm(100)
y <- x - 2 * x^2 + rnorm(100)
```
The n is 100 and the p is 1.
The model is 
$$
Y = X - 2X^2 + \epsilon
$$
## b
```{r, 8b}
plot(x, y)
```
The plot suggests that there is a curved relationship between x and y. When x is less than 0.25, y increases as x increases; when x is larger than 0.25, y decreases as x increases. 

## c
```{r, 8c}
#i
library(boot)
set.seed(1)
Data <- data.frame(x, y)
fit.glm.1 <- glm(y ~ x)
cv.glm(Data, fit.glm.1)$delta[1]

#ii
fit.glm.2 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.glm.2)$delta[1]

#iii
fit.glm.3 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit.glm.3)$delta[1]

#iv
fit.glm.4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit.glm.4)$delta[1]
```
## d
```{r, 8d}
#i
set.seed(10)
fit.glm.1 <- glm(y ~ x)
cv.glm(Data, fit.glm.1)$delta[1]

#ii
fit.glm.2 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.glm.2)$delta[1]

#iii
fit.glm.3 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit.glm.3)$delta[1]

#iv
fit.glm.4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit.glm.4)$delta[1]

```
The results are the same. This is because you always leave out one observation, fit the model, and predict, there's no randomness involved in LOOCV once you have the data. Therefore, the LOOCV errors for a given dataset will always be the same regardless of the random seed.

## e
The LOOCV estimate for the test MSE is minimum for “fit.glm.2”. Yes, this is what I expect because as we can see in (a) and (b) the relation between “x” and “y” is quadratic.

## f
```{r, 8f}
summary(fit.glm.4)
```
The summary shows that the linear and quadratic terms are statistically significants and that the cubic and 4th degree terms are not statistically significants. As the LOOCV error was lowest for the quadratic model, this is in agreement with the conclusion from the hypothesis tests that the significant terms are the intercept, linear, and quadratic ones.

# Applied 5-9
## a
```{r, 9a}
library(MASS)
attach(Boston)
mu.hat <- mean(medv)
mu.hat
```

## b
```{r, 9b}
se.hat <- sd(medv) / sqrt(length(Boston$medv))
se.hat
```
The standard error of mu-hat is 0.4088611. This means that, on average, we would expect the sample mean of "medv" to deviate from the true population mean by about 0.4088611 units. 

## c
```{r, 9c}
set.seed(1)
boot.fn <- function(data, index) {
    mu <- mean(data[index])
    return (mu)
}
boot(medv, boot.fn, 1000)
```
The standard error we got this time is 0.4107, which is very close to the standard error we got in (b).

## d
```{r, 9d}
t.test(medv)
CI.mu.hat <- c(22.53 - 2 * 0.4107, 22.53 + 2 * 0.4107)
CI.mu.hat
```
The bootstrap confidence interval is very close to the one provided by the t.test() function.

## e
```{r, 9e}
med.hat <- median(medv)
med.hat
```

## f
```{r, 9f}
boot.fn <- function(data, index) {
    mu <- median(data[index])
    return (mu)
}
boot(medv, boot.fn, 1000)
```
We get an estimated median value of 21.2 which is equal to the value obtained in (e). And a standard error of 0.3688 which is relatively small compared to median value. This suggests that the median value is estimated with a good degree of precision.

## g
```{r, 9g}
percent.hat <- quantile(medv, c(0.1))
percent.hat
```

## h
```{r, 9h}
boot.fn <- function(data, index) {
    mu <- quantile(data[index], c(0.1))
    return (mu)
}
boot(medv, boot.fn, 1000)
```
In this question, we get an estimated tenth percentile value of 12.75 which is  equal to the value we got in (g), with a standard error of 0.5113 which is relatively small compared to percentile value. This suggests that the tenth percentile value is estimated with a good degree of precision.




